<p align="center">
  <img src="assets/logo.png" height=200>
</p>
<hr>
<div align="center">
  
## SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model

<p align="center">
  <a href="[https://aim-uofa.github.io/MovieDreamer/](https://seq-afford.github.io/)"><b>📖 Project Page</b></a> |
  <a href="https://arxiv.org/pdf/2412.01550"><b>📄 Paper Link</b></a> |
</p>

</div>

> We introduce SeqAfford, a Multi-Modal Language Model (MLLM) capable of serialized affordance inference implied in human instructions: 1) Single Affordance Reasoning; 2) Sequential Affordance Reasoning; 3) Sequential Affordance Reasoning with Multiple Objects

<div align="center">
    <img src="assets/demo.png" height=500>
</div>

## 📣 News
- [7/20/2024] Paper released!

## 😲 Results
Please refer to our [homepage](https://aim-uofa.github.io/MovieDreamer/) for more thrilling results!


## 🛠️ Setup
- Comming Soon...


## 🚩 Plan
- [x] Story Results and Video Results.
- [x] Paper Released.
- [ ] Source Code and Pretrained Weights of Diffusion AutoEncoder.
- [ ] Source Code and Pretrained Weights of Autoregressive Model.
- [ ] Source code of Video Generation.
- [ ] Source code of Data Pre-processing.
- [ ] Source code of Training.
<!-- --- -->


## 🎫 License

For academic use, this project is licensed under [the 2-clause BSD License](https://opensource.org/license/bsd-2-clause). 
For commercial use, please contact [Chunhua Shen](mailto:chhshen@gmail.com).

## 🖊️ Citation
```
@misc{zhao2024moviedreamerhierarchicalgenerationcoherent,
      title={MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence}, 
      author={Canyu Zhao and Mingyu Liu and Wen Wang and Jianlong Yuan and Hao Chen and Bo Zhang and Chunhua Shen},
      year={2024},
      eprint={2407.16655},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.16655}, 
}
```
